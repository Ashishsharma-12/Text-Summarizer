{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy networkx\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\tensorflow\\Lib\\site-packages\\cupy\\_environment.py:541: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox, ttk\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dont use this in collab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizerApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Summarizer\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.configure(bg=\"#f5f5f5\")\n",
    "        \n",
    "        # Initialize spaCy model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            messagebox.showerror(\"Model Error\", \"Please install the spaCy model by running:\\npython -m spacy download en_core_web_sm\")\n",
    "            root.destroy()\n",
    "            return\n",
    "        \n",
    "        self.create_widgets()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        # Header\n",
    "        header_frame = tk.Frame(self.root, bg=\"#f5f5f5\")\n",
    "        header_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "        \n",
    "        tk.Label(\n",
    "            header_frame, \n",
    "            text=\"TextRank Summarizer\", \n",
    "            font=(\"Arial\", 18, \"bold\"),\n",
    "            bg=\"#f5f5f5\"\n",
    "        ).pack(side=tk.LEFT)\n",
    "        \n",
    "        # Main content\n",
    "        content_frame = tk.Frame(self.root, bg=\"#f5f5f5\")\n",
    "        content_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Left panel for input\n",
    "        left_frame = tk.LabelFrame(content_frame, text=\"Original Text\", bg=\"#f5f5f5\", font=(\"Arial\", 10))\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        self.input_text = scrolledtext.ScrolledText(left_frame, wrap=tk.WORD, font=(\"Arial\", 11))\n",
    "        self.input_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Right panel for output\n",
    "        right_frame = tk.LabelFrame(content_frame, text=\"Summary\", bg=\"#f5f5f5\", font=(\"Arial\", 10))\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        self.output_text = scrolledtext.ScrolledText(right_frame, wrap=tk.WORD, font=(\"Arial\", 11))\n",
    "        self.output_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Controls panel\n",
    "        controls_frame = tk.Frame(self.root, bg=\"#f5f5f5\")\n",
    "        controls_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "        \n",
    "        # Summary length selection\n",
    "        tk.Label(\n",
    "            controls_frame, \n",
    "            text=\"Summary length:\", \n",
    "            bg=\"#f5f5f5\", \n",
    "            font=(\"Arial\", 10)\n",
    "        ).pack(side=tk.LEFT, padx=(0, 5))\n",
    "        \n",
    "        self.summary_percent = tk.StringVar(value=\"30%\")\n",
    "        summary_options = [\"10%\", \"20%\", \"30%\", \"40%\", \"50%\"]\n",
    "        summary_dropdown = ttk.Combobox(\n",
    "            controls_frame, \n",
    "            textvariable=self.summary_percent, \n",
    "            values=summary_options, \n",
    "            width=5,\n",
    "            state=\"readonly\"\n",
    "        )\n",
    "        summary_dropdown.pack(side=tk.LEFT, padx=(0, 10))\n",
    "        \n",
    "        # Buttons\n",
    "        self.summarize_btn = tk.Button(\n",
    "            controls_frame, \n",
    "            text=\"Summarize\", \n",
    "            command=self.summarize_text,\n",
    "            bg=\"#4CAF50\", \n",
    "            fg=\"white\",\n",
    "            font=(\"Arial\", 10, \"bold\"),\n",
    "            padx=15,\n",
    "            relief=tk.RAISED,\n",
    "            borderwidth=2\n",
    "        )\n",
    "        self.summarize_btn.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.clear_btn = tk.Button(\n",
    "            controls_frame, \n",
    "            text=\"Clear\", \n",
    "            command=self.clear_fields,\n",
    "            bg=\"#f44336\", \n",
    "            fg=\"white\",\n",
    "            font=(\"Arial\", 10, \"bold\"),\n",
    "            padx=15,\n",
    "            relief=tk.RAISED,\n",
    "            borderwidth=2\n",
    "        )\n",
    "        self.clear_btn.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = tk.Label(\n",
    "            self.root, \n",
    "            textvariable=self.status_var, \n",
    "            anchor=tk.W, \n",
    "            bg=\"#e0e0e0\", \n",
    "            relief=tk.SUNKEN,\n",
    "            padx=5\n",
    "        )\n",
    "        self.status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    def summarize_text(self):\n",
    "        \"\"\"Generate a summary of the input text using TextRank\"\"\"\n",
    "        text = self.input_text.get(\"1.0\", tk.END).strip()\n",
    "        \n",
    "        if not text:\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter text to summarize.\")\n",
    "            return\n",
    "        \n",
    "        # Update status\n",
    "        self.status_var.set(\"Summarizing...\")\n",
    "        self.root.update_idletasks()\n",
    "        \n",
    "        try:\n",
    "            # Parse percentage\n",
    "            percent = int(self.summary_percent.get().strip('%'))\n",
    "            summary = self.textrank_summary(text, percent/100)\n",
    "            \n",
    "            # Display summary\n",
    "            self.output_text.delete(\"1.0\", tk.END)\n",
    "            self.output_text.insert(tk.END, summary)\n",
    "            \n",
    "            # Update status\n",
    "            self.status_var.set(f\"Summary complete. ({len(summary.split())} words)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "            self.status_var.set(\"Error occurred during summarization.\")\n",
    "    \n",
    "    def textrank_summary(self, text, per):\n",
    "        \"\"\"Generate text summary using TextRank algorithm\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        if len(sentences) <= 1:\n",
    "            return text\n",
    "        \n",
    "        # Create sentence vectors using spaCy's word vectors\n",
    "        sentence_vectors = []\n",
    "        for sent in sentences:\n",
    "            # Skip sentences with no words with vectors\n",
    "            if not any(token.has_vector for token in sent):\n",
    "                sent_vec = np.zeros((len(sent), 96))  # Default embedding dimension\n",
    "            else:\n",
    "                words_with_vectors = [token.vector for token in sent if token.has_vector]\n",
    "                if not words_with_vectors:\n",
    "                    sent_vec = np.zeros(96)  # Default dimension\n",
    "                else:\n",
    "                    sent_vec = np.mean(words_with_vectors, axis=0)\n",
    "            sentence_vectors.append(sent_vec)\n",
    "        \n",
    "        # Create similarity matrix\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "        \n",
    "        # Fill the similarity matrix\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    # Make sure we don't divide by zero\n",
    "                    if np.linalg.norm(sentence_vectors[i]) * np.linalg.norm(sentence_vectors[j]) == 0:\n",
    "                        sim_mat[i][j] = 0\n",
    "                    else:\n",
    "                        sim_mat[i][j] = self._cosine_similarity(sentence_vectors[i], sentence_vectors[j])\n",
    "        \n",
    "        # Create networkx graph and add edges with weights\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        \n",
    "        # Apply PageRank algorithm\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        \n",
    "        # Sort sentences by score and select top sentences\n",
    "        ranked_sentences = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)\n",
    "        \n",
    "        # Calculate the number of sentences for the summary\n",
    "        summary_size = max(1, int(len(sentences) * per))\n",
    "        \n",
    "        # Get top N sentences and sort them by original position\n",
    "        top_sentences = sorted(ranked_sentences[:summary_size], key=lambda x: x[1])\n",
    "        \n",
    "        # Combine sentences into summary\n",
    "        summary = \" \".join([s.text for _, _, s in top_sentences])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        # Handle zero vectors\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            return 0\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    def clear_fields(self):\n",
    "        \"\"\"Clear input and output text fields\"\"\"\n",
    "        self.input_text.delete(\"1.0\", tk.END)\n",
    "        self.output_text.delete(\"1.0\", tk.END)\n",
    "        self.status_var.set(\"Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = TextSummarizerApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Application error: {str(e)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***USE this code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizer:\n",
    "    def __init__(self):\n",
    "        # Initialize spaCy model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Error: Please install the spaCy model by running:\")\n",
    "            print(\"python -m spacy download en_core_web_sm\")\n",
    "            raise\n",
    "    \n",
    "    def summarize(self, text, percent=0.3):\n",
    "        \"\"\"\n",
    "        Generate a summary of the given text using TextRank algorithm\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to summarize\n",
    "            percent (float): Percentage of original text to include in summary (0.1 to 0.5)\n",
    "            \n",
    "        Returns:\n",
    "            str: The generated summary\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            return \"No text provided for summarization.\"\n",
    "            \n",
    "        # Make sure percent is between 0.1 and 0.5\n",
    "        percent = max(0.1, min(0.5, percent))\n",
    "        \n",
    "        return self.textrank_summary(text, percent)\n",
    "    \n",
    "    def textrank_summary(self, text, per):\n",
    "        \"\"\"Generate text summary using TextRank algorithm\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        if len(sentences) <= 1:\n",
    "            return text\n",
    "        \n",
    "        # Create sentence vectors using spaCy's word vectors\n",
    "        sentence_vectors = []\n",
    "        for sent in sentences:\n",
    "            # Skip sentences with no words with vectors\n",
    "            if not any(token.has_vector for token in sent):\n",
    "                sent_vec = np.zeros((len(sent), 96))  # Default embedding dimension\n",
    "            else:\n",
    "                words_with_vectors = [token.vector for token in sent if token.has_vector]\n",
    "                if not words_with_vectors:\n",
    "                    sent_vec = np.zeros(96)  # Default dimension\n",
    "                else:\n",
    "                    sent_vec = np.mean(words_with_vectors, axis=0)\n",
    "            sentence_vectors.append(sent_vec)\n",
    "        \n",
    "        # Create similarity matrix\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "        \n",
    "        # Fill the similarity matrix\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    # Make sure we don't divide by zero\n",
    "                    if np.linalg.norm(sentence_vectors[i]) * np.linalg.norm(sentence_vectors[j]) == 0:\n",
    "                        sim_mat[i][j] = 0\n",
    "                    else:\n",
    "                        sim_mat[i][j] = self._cosine_similarity(sentence_vectors[i], sentence_vectors[j])\n",
    "        \n",
    "        # Create networkx graph and add edges with weights\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        \n",
    "        # Apply PageRank algorithm\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        \n",
    "        # Sort sentences by score and select top sentences\n",
    "        ranked_sentences = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)\n",
    "        \n",
    "        # Calculate the number of sentences for the summary\n",
    "        summary_size = max(1, int(len(sentences) * per))\n",
    "        \n",
    "        # Get top N sentences and sort them by original position\n",
    "        top_sentences = sorted(ranked_sentences[:summary_size], key=lambda x: x[1])\n",
    "        \n",
    "        # Combine sentences into summary\n",
    "        summary = \" \".join([s.text for _, _, s in top_sentences])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def _cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        # Handle zero vectors\n",
    "        if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "            return 0\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text Length: 181 words\n",
      "Summary Length: 74 words\n",
      "\n",
      "--- SUMMARY ---\n",
      "\n",
      "This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
      "     As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. \n",
      "     For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample text for demonstration\n",
    "    sample_text = \"\"\"\n",
    "    Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. \n",
    "    AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
    "    The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\". \n",
    "    This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
    "    AI applications include advanced web search engines, recommendation systems, understanding human speech, self-driving cars, automated decision-making and competing at the highest level in strategic game systems.\n",
    "    As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. \n",
    "    For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a summarizer\n",
    "    summarizer = TextSummarizer()\n",
    "    \n",
    "    # Generate summary at 30% length\n",
    "    summary = summarizer.summarize(sample_text, 0.5)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Original Text Length:\", len(sample_text.split()), \"words\")\n",
    "    print(\"Summary Length:\", len(summary.split()), \"words\")\n",
    "    print(\"\\n--- SUMMARY ---\\n\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Extract text from a PDF file and return chunks with metadata.\"\"\"\n",
    "    reader = PdfReader(pdf_file)\n",
    "    chunks = []\n",
    "    pdf_name = pdf_file.name\n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text() or \"\"\n",
    "        if text.strip():\n",
    "            chunks.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\"pdf_file\": pdf_name, \"page_number\": page_num}\n",
    "            })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamlitAPIException",
     "evalue": "The value `\":material​/Summarize:\"` is not a valid Material icon. Please use a Material icon shortcode like **`:material​/thumb_up:`**. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamlitAPIException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     st\u001b[38;5;241m.\u001b[39mset_page_config(page_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText Summarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, page_icon\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:material/Summarize:\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m      4\u001b[0m     st\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText Summarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m st\u001b[38;5;241m.\u001b[39msidebar():\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tensorflow\\Lib\\site-packages\\streamlit\\runtime\\metrics_util.py:409\u001b[0m, in \u001b[0;36mgather_metrics.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to collect command telemetry\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39mex)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     result \u001b[38;5;241m=\u001b[39m non_optional_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RerunException \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# Duplicated from below, because static analysis tools get confused\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;66;03m# by deferring the rethrow.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracking_activated \u001b[38;5;129;01mand\u001b[39;00m command_telemetry:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tensorflow\\Lib\\site-packages\\streamlit\\commands\\page_config.py:238\u001b[0m, in \u001b[0;36mset_page_config\u001b[1;34m(page_title, page_icon, layout, initial_sidebar_state, menu_items)\u001b[0m\n\u001b[0;32m    235\u001b[0m     msg\u001b[38;5;241m.\u001b[39mpage_config_changed\u001b[38;5;241m.\u001b[39mtitle \u001b[38;5;241m=\u001b[39m page_title\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m page_icon \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     msg\u001b[38;5;241m.\u001b[39mpage_config_changed\u001b[38;5;241m.\u001b[39mfavicon \u001b[38;5;241m=\u001b[39m _get_favicon_string(page_icon)\n\u001b[0;32m    240\u001b[0m pb_layout: PageConfigProto\u001b[38;5;241m.\u001b[39mLayout\u001b[38;5;241m.\u001b[39mValueType\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentered\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tensorflow\\Lib\\site-packages\\streamlit\\commands\\page_config.py:107\u001b[0m, in \u001b[0;36m_get_favicon_string\u001b[1;34m(page_icon)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m page_icon\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(page_icon, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m page_icon\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:material\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_material_icon(page_icon)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Convert Path to string if necessary\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(page_icon, Path):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\tensorflow\\Lib\\site-packages\\streamlit\\string_util.py:110\u001b[0m, in \u001b[0;36mvalidate_material_icon\u001b[1;34m(maybe_material_icon)\u001b[0m\n\u001b[0;32m    103\u001b[0m pack_name, icon_name \u001b[38;5;241m=\u001b[39m icon_match\u001b[38;5;241m.\u001b[39mgroups()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    106\u001b[0m     pack_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_icon_packs\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m icon_name\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_material_icon(icon_name)\n\u001b[0;32m    109\u001b[0m ):\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StreamlitAPIException(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe value `\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaybe_material_icon\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39minvisible_white_space\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` is not a valid Material icon.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use a Material icon shortcode like **`:material\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvisible_white_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/thumb_up:`**. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpack_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00micon_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mStreamlitAPIException\u001b[0m: The value `\":material​/Summarize:\"` is not a valid Material icon. Please use a Material icon shortcode like **`:material​/thumb_up:`**. "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    st.set_page_config(page_title=\"Text Summarizer\", page_icon=\":material/Summarize:\")    \n",
    "\n",
    "    st.title(\"Text Summarizer\")\n",
    "\n",
    "    with st.sidebar():\n",
    "        uploaded_file = st.file_uploader(\"Choose a file\", accept_multiple_files=False, type=['pdf'])\n",
    "\n",
    "        if uploaded_file:\n",
    "             text = extract_text_from_pdf(uploaded_file)\n",
    "             st.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
